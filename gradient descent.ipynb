{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Descent Algorithm "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "通过python实例，对比阐述Batch Gradient Descent 与 Stochastic Gradient Densent 之间的异同."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Code 1. Stochastic Gradient Densent Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding = utf-8\n",
    "\n",
    "#Training data set\n",
    "#each element in x represents (x0, x1, x2)\n",
    "x = [(1, 0., 3), (1, 1., 3), (1, 2., 3), (1, 3., 2), (1, 4., 4)]\n",
    "#y[i] is the output of y = theta0*x[0] + theta1* x[1] + theta2*x[2]\n",
    "y = [95.364,97.217205,75.195834,60.105519,49.342380]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Initialization the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "alpha   = 0.1    # learning rate\n",
    "diff    = 0\n",
    "error0  = 0\n",
    "error1  = 0\n",
    "m = len(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#initialization the coeffiencts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta0 = 0\n",
    "theta1 = 0\n",
    "theta2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: theta0 : 98.770497, theta1 : -11.374639, theta2 : 0.115204\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # calculate the coeffiencts\n",
    "    for i in xrange(m):\n",
    "        diff = y[i] - (theta0*x[i][0] + theta1*x[i][1] + theta2*x[i][2])\n",
    "        \n",
    "        theta0 = theta0 + alpha*diff*x[i][0]\n",
    "        theta1 = theta1 + alpha*diff*x[i][1]\n",
    "        theta2 = theta2 + alpha*diff*x[i][2]\n",
    "    \n",
    "    # calculate the cost function\n",
    "    error1 = 0\n",
    "    for lp in xrange(m):\n",
    "        error1 = error1 + (y[i] - (theta0*x[i][0] + theta1*x[i][1] + theta2*x[i][2]))**2/2\n",
    "    if abs(error1 - error0) < epsilon:\n",
    "        break\n",
    "    else:\n",
    "        error0 = error1\n",
    "    \n",
    "    #print 'theta0 : %f, theta1 : %f, theta2 : %f' % (theta0, theta1, theta2)\n",
    "\n",
    "print \"Done: theta0 : %f, theta1 : %f, theta2 : %f\" % (theta0, theta1, theta2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Code 2. Batch gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "alpha   = 0.001    # learning rate\n",
    "diff    = 0\n",
    "error0  = 0\n",
    "error1  = 0\n",
    "m = len(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#initialization the coeffiencts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta0 = 0\n",
    "theta1 = 0\n",
    "theta2 = 0\n",
    "sum0   = 0\n",
    "sum1   = 0\n",
    "sum2   = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: theta0 : 97.821404, theta1 : -13.026773, theta2 : 1.222080\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    #calculate the parameters\n",
    "    for i in xrange(m):\n",
    "        #begin batch gradient descent\n",
    "        diff = y[i] - (theta0*x[i][0] + theta1*x[i][1] + theta2*x[i][2])\n",
    "        sum0 = sum0 + alpha * diff * x[i][0]\n",
    "        sum1 = sum1 + alpha * diff * x[i][1]\n",
    "        sum2 = sum2 + alpha * diff * x[i][2]\n",
    "        # end batch gradient descent\n",
    "    theta0 = sum0\n",
    "    theta1 = sum1\n",
    "    theta2 = sum2\n",
    "    \n",
    "    #calculate the cost function\n",
    "    error1 = 0\n",
    "    for lp in xrange(len(x)):\n",
    "        error1 += (y[i]-( theta0*x[i][0] + theta1*x[i][1] + theta2*x[i][2]))**2/2\n",
    "    \n",
    "    if abs(error1 - error0) < epsilon:\n",
    "        break\n",
    "    else:\n",
    "        error0 = error1\n",
    "    #print ' theta0 : %f, theta1 : %f, theta2 : %f, error1 : %f'%(theta0,theta1,theta2,error1)\n",
    "    \n",
    "print 'Done: theta0 : %f, theta1 : %f, theta2 : %f'%(theta0,theta1,theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两者区别："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Stochastic Gradient Densent Algorithm：在迭代的时候，每迭代一个新的样本，就会更新一次所有的theta参数.\n",
    "2. Batch Gradient Descent Algorithm：在迭代的时候，是在完成所有的样本后才会更新一次theta参数."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "因此当样本数量很大时候，批梯度得做完所有样本的计算才能更新一次theta，从而花费的时间远大于随机梯度下降。但是随机梯度下降过早的结束了迭代，使得它获取的值只是接近局部最优解，而并非像批梯度下降算法那么是局部最优解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
